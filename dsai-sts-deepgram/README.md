# Digital Storming AI - Deepgram Speech-to-Speech Integration

[![Discord](https://img.shields.io/discord/1347239846632226998?label=Discord&logo=discord)](https://discord.gg/DFTU69Hg74)
[![GitHub Repo stars](https://img.shields.io/github/stars/agentvoiceresponse/dsai-sts-deepgram?style=social)](https://github.com/agentvoiceresponse/dsai-sts-deepgram)
[![Docker Pulls](https://img.shields.io/docker/pulls/agentvoiceresponse/dsai-sts-deepgram?label=Docker%20Pulls&logo=docker)](https://hub.docker.com/r/agentvoiceresponse/dsai-sts-deepgram)
[![Ko-fi](https://img.shields.io/badge/Support%20us%20on-Ko--fi-ff5e5b.svg)](https://ko-fi.com/agentvoiceresponse)

This repository showcases the integration between **Digital Storming AI** and **Deepgram's Speech-to-Speech API**. The application leverages Deepgram's powerful speech processing capabilities to provide intelligent, context-aware responses in real-time audio format.

## Prerequisites

To set up and run this project, you will need:

1. **Node.js** and **npm** installed
2. A **Deepgram API key** with access to the Speech-to-Speech API
3. **WebSocket** support in your environment

## Setup

### 1. Clone the Repository

```bash
git clone https://github.com/agentvoiceresponse/dsai-sts-deepgram.git
cd dsai-sts-deepgram
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Configure Environment Variables

**When running in DSAI (Digital Storming AI):**
- **All configuration comes from the database** via the backend API
- The container fetches configuration from the database on each new call
- No `.env` file is needed - values are stored in the DSAI database
- Changes in the database are picked up automatically without container restart
- See the [DSAI Configuration](#avr-configuration) section below

**For standalone testing (outside DSAI):**
Create a `.env` file in the root of the project to store your API keys and configuration:

```bash
DEEPGRAM_API_KEY=your_deepgram_api_key
AGENT_PROMPT="Your custom agent prompt here..."
PORT=6033
DEEPGRAM_SAMPLE_RATE=8000
DEEPGRAM_ASR_MODEL=nova-3
DEEPGRAM_TTS_MODEL=aura-2-thalia-en
DEEPGRAM_GREETING="Hi there, I'm your virtual assistant—how can I help today?"
OPENAI_MODEL=gpt-4o-mini
```

**Required Variables:**

- `DEEPGRAM_API_KEY`: Your Deepgram API key
- `AGENT_PROMPT`: The system prompt for the AI agent behavior

**Optional Variables:**

- `PORT`: Server port (default: 6033)
- `DEEPGRAM_SAMPLE_RATE`: Audio sample rate (default: 8000)
- `DEEPGRAM_ASR_MODEL`: Speech recognition model (default: nova-3)
- `DEEPGRAM_TTS_MODEL`: Text-to-speech model (default: aura-2-thalia-en)
- `DEEPGRAM_GREETING`: Initial greeting message
- `OPENAI_MODEL`: OpenAI model for responses (default: gpt-4o-mini)

### 4. Running the Application

Start the application by running the following command:

```bash
node index.js
```

The server will start on the port defined in the environment variable (default: 6033).

## How It Works

The **Digital Storming AI** system integrates with Deepgram's Speech-to-Speech API to provide intelligent audio-based responses to user queries. The server receives audio input from users, forwards it to Deepgram's API, and then returns the model's response as audio in real-time using WebSocket communication.

### Key Components

- **Express.js Server**: Handles incoming audio streams from clients
- **WebSocket Communication**: Manages real-time communication with Deepgram's API
- **Audio Processing**: Handles audio format conversion and streaming
- **Real-time Streaming**: Processes and streams audio data in real-time

### Audio Processing

The application is configured to work with:

- **Input Audio**: 16-bit PCM at 8kHz
- **Output Audio**: 16-bit PCM at 8kHz
- **Encoding**: Linear16 format

## API Endpoints

### POST `/speech-to-speech-stream`

This endpoint accepts an audio stream and returns a streamed audio response generated by Deepgram.

**Request:**

- Content-Type: audio/x-raw
- Format: 16-bit PCM at 8kHz
- Method: POST

**Response:**

- Content-Type: text/event-stream
- Format: 16-bit PCM at 8kHz
- Streamed audio data in real-time

## DSAI Configuration (Database-Driven)

When running in DSAI, **all configuration is stored in the database** and fetched dynamically:

1. **Configuration Source**: Database (via backend API)
2. **Real-Time Updates**: Changes in database are picked up **immediately** on each new call (100ms cache)
3. **No Container Restart**: Update values in the DSAI dashboard and they apply to new calls instantly
4. **Fallback**: If API is unavailable, uses environment variables passed by backend (which also come from database)

### How It Works

1. Backend reads provider configuration from database
2. Backend passes `PROVIDER_ID` and `BACKEND_URL` to container
3. Container fetches latest config from: `GET /internal/providers/{PROVIDER_ID}/config`
4. Configuration is cached for only 100ms to ensure real-time updates while preventing excessive API calls
5. **Each new call uses fresh configuration from database** - prompt and greeting changes apply immediately

### Real-Time Updates

- ✅ **Prompt changes**: Update `AGENT_PROMPT` in dashboard → applies to next call immediately
- ✅ **Greeting changes**: Update `DEEPGRAM_GREETING` in dashboard → applies to next call immediately  
- ✅ **All settings**: Any provider configuration change applies to new calls within 100ms
- ✅ **No restart needed**: Changes are picked up automatically on each new connection

### Benefits

- ✅ No container restart needed for configuration changes
- ✅ All values stored in database (single source of truth)
- ✅ Changes apply to new calls immediately (real-time updates)
- ✅ No `.env` file management required

## Customizing the Application

### Environment Variables (Standalone Mode Only)

For standalone testing (not in DSAI), you can customize the application behavior using the following environment variables:

**Required:**

- `DEEPGRAM_API_KEY`: Your Deepgram API key
- `AGENT_PROMPT`: The system prompt that defines the AI agent's behavior and personality

**Optional:**

- `PORT`: The port on which the server will listen (default: 6033)
- `DEEPGRAM_SAMPLE_RATE`: Audio sample rate in Hz (default: 8000)
- `DEEPGRAM_ASR_MODEL`: Deepgram ASR model (default: nova-3)
- `DEEPGRAM_TTS_MODEL`: Deepgram TTS model (default: aura-2-thalia-en)
- `DEEPGRAM_GREETING`: Initial greeting message
- `OPENAI_MODEL`: OpenAI model for generating responses (default: gpt-4o-mini)

## Error Handling

The application includes comprehensive error handling for:

- WebSocket connection issues
- Audio processing errors
- Deepgram API errors
- Stream processing errors

All errors are logged to the console and appropriate error messages are returned to the client.

## Contributors

We would like to express our gratitude to all the contributors who have helped make this project possible:

- [Mirko Bertone](https://github.com/mirkobertone) - For their valuable contributions and support

## Support & Community

- **GitHub:** [https://github.com/agentvoiceresponse](https://github.com/agentvoiceresponse) - Report issues, contribute code.
- **Discord:** [https://discord.gg/DFTU69Hg74](https://discord.gg/DFTU69Hg74) - Join the community discussion.
- **Docker Hub:** [https://hub.docker.com/u/agentvoiceresponse](https://hub.docker.com/u/agentvoiceresponse) - Find Docker images.
- **NPM:** [https://www.npmjs.com/~agentvoiceresponse](https://www.npmjs.com/~agentvoiceresponse) - Browse our packages.
- **Wiki:** [https://wiki.agentvoiceresponse.com/en/home](https://wiki.agentvoiceresponse.com/en/home) - Project documentation and guides.

## Support DSAI

DSAI is free and open-source. If you find it valuable, consider supporting its development:

<a href="https://ko-fi.com/agentvoiceresponse" target="_blank"><img src="https://ko-fi.com/img/githubbutton_sm.svg" alt="Support us on Ko-fi"></a>

## License

MIT License - see the [LICENSE](LICENSE.md) file for details.
