services:
  dsai-core:
    image: agentvoiceresponse/dsai-core
    platform: linux/x86_64
    container_name: dsai-core
    restart: always
    environment:
      - PORT=5001
      - ASR_URL=http://dsai-asr-vosk:6010/speech-to-text-stream
      - LLM_URL=http://dsai-llm-openai:6002/prompt-stream
      - TTS_URL=http://dsai-tts-kokoro:6012/text-to-speech-stream
      - INTERRUPT_LISTENING=true
      - SYSTEM_MESSAGE="Hello, how can I help you today?"
      - WEBHOOK_URL=${WEBHOOK_URL:-http://host.docker.internal:3001/webhooks}
      - WEBHOOK_SECRET=${WEBHOOK_SECRET:-}
    ports:
      - 5001:5001
    networks:
      - dsai

  dsai-asr-vosk:
    image: agentvoiceresponse/dsai-asr-vosk
    platform: linux/x86_64
    container_name: dsai-asr-vosk
    restart: always
    environment:
      - PORT=6010
      - MODEL_PATH=model
    volumes:
      - ./model:/usr/src/app/model
    networks:
      - dsai

  dsai-tts-kokoro:
    image: agentvoiceresponse/dsai-tts-kokoro
    platform: linux/x86_64
    container_name: dsai-tts-kokoro
    restart: always
    environment:
      - PORT=6012
      - KOKORO_BASE_URL=http://dsai-kokoro:8880
      - KOKORO_VOICE=${KOKORO_VOICE:-af_alloy}
      - KOKORO_SPEED=${KOKORO_SPEED:-1.3}
    networks:
      - dsai

  dsai-kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-cpu
    container_name: dsai-kokoro
    restart: always
    ports:
      - 8880:8880
    networks:
      - dsai

  dsai-llm-openai:
    image: agentvoiceresponse/dsai-llm-openai
    platform: linux/x86_64
    container_name: dsai-llm-openai
    restart: always
    environment:
      - PORT=6002
      - OPENAI_BASEURL=http://dsai-ollama:11434/v1
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - OPENAI_MODEL=tinyllama
      - OPENAI_MAX_TOKENS=${OPENAI_MAX_TOKENS:-100}
      - OPENAI_TEMPERATURE=${OPENAI_TEMPERATURE:-0.0}
      - AMI_URL=${AMI_URL:-http://dsai-ami:6006}
      - SYSTEM_PROMPT="You are a helpful assistant."
    networks:
      - dsai

  dsai-ollama:
    image: ollama/ollama
    container_name: dsai-ollama
    restart: always
    ports:
      - 11434:11434
    volumes:
      - ./ollama:/root/.ollama
    networks:
      - dsai

  dsai-ollama-web:
    image: ghcr.io/open-webui/open-webui
    container_name: dsai-ollama-web
    restart: always
    ports:
      - 3001:8080
    volumes:
      - ./ollama-web:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://dsai-ollama:11434
    networks:
      - dsai

  dsai-asterisk:
    image: agentvoiceresponse/dsai-asterisk
    platform: linux/x86_64
    container_name: dsai-asterisk
    restart: always
    ports:
      - 5038:5038
      - 5060:5060
      - 9088:8088
      - 9089:8089
      - 9090:8090
      - 10000-10050:10000-10050/udp
    volumes:
      - ./asterisk/conf/manager.conf:/etc/asterisk/my_manager.conf
      - ./asterisk/conf/pjsip.conf:/etc/asterisk/my_pjsip.conf
      - ./asterisk/conf/extensions.conf:/etc/asterisk/my_extensions.conf
      - ./asterisk/conf/queues.conf:/etc/asterisk/my_queues.conf
      - ./asterisk/conf/ari.conf:/etc/asterisk/my_ari.conf
      - ./asterisk/recordings:/var/spool/asterisk/monitor
    networks:
      - dsai

  dsai-ami:
    image: agentvoiceresponse/dsai-ami
    platform: linux/x86_64
    container_name: dsai-ami
    restart: always
    environment:
      - PORT=6006
      - AMI_HOST=${AMI_HOST:-dsai-asterisk}
      - AMI_PORT=${AMI_PORT:-5038}
      - AMI_USERNAME=${AMI_USERNAME:-dsai}
      - AMI_PASSWORD=${AMI_PASSWORD:-dsai}
    ports:
      - 6006:6006
    networks:
      - dsai

networks:
  dsai:
    name: dsai
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
